{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# Importing libraries\n",
        "# ===============================================================\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import clip\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "dataRoot = \"/content/retina/retina\"\n",
        "\n",
        "# ===============================================================\n",
        "# Preparing CLIP model\n",
        "# ===============================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clipModel, clipPreprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "clipModel.eval()\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Retina dataset class\n",
        "# ===============================================================\n",
        "class RetinaDataset(Dataset):\n",
        "    def __init__(self, rootDir):\n",
        "        self.samples = []\n",
        "        self.preprocess = clipPreprocess\n",
        "\n",
        "        # Scanning folders and building sample list\n",
        "        for clsName in os.listdir(rootDir):\n",
        "            clsPath = os.path.join(rootDir, clsName)\n",
        "            imgPath = os.path.join(clsPath, \"img\")\n",
        "\n",
        "            if not os.path.isdir(imgPath):\n",
        "                continue\n",
        "\n",
        "            label = 0 if clsName.lower() == \"good\" else 1\n",
        "\n",
        "            for f in os.listdir(imgPath):\n",
        "                if f.lower().endswith((\"png\", \"jpg\", \"jpeg\")):\n",
        "                    self.samples.append((os.path.join(imgPath, f), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        return self.preprocess(img), label\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Creating loaders\n",
        "# ===============================================================\n",
        "validRoot = os.path.join(dataRoot, \"valid\")\n",
        "testRoot  = os.path.join(dataRoot, \"test\")\n",
        "\n",
        "trainDs = RetinaDataset(validRoot)\n",
        "testDs  = RetinaDataset(testRoot)\n",
        "\n",
        "trainLoader = DataLoader(trainDs, batch_size=32, shuffle=True)\n",
        "testLoader  = DataLoader(testDs, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Train samples:\", len(trainDs))\n",
        "print(\"Test samples:\", len(testDs))\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Building multi-prompt text templates\n",
        "# ===============================================================\n",
        "normalTemplates = [\n",
        "    \"a healthy retinal OCT image\", \"a normal retinal OCT scan\",\n",
        "    \"a retina with no abnormalities\", \"a clean retinal OCT image\",\n",
        "    \"a normal eye OCT\", \"a retinal OCT showing no disease\",\n",
        "    \"a healthy retina\", \"normal retinal tissue OCT\",\n",
        "    \"a medical OCT of a healthy retina\", \"healthy retinal structure\"\n",
        "]\n",
        "\n",
        "anomalyTemplates = [\n",
        "    \"an abnormal retinal OCT image\", \"a diseased retina OCT\",\n",
        "    \"a retinal OCT showing pathology\", \"a damaged retinal OCT\",\n",
        "    \"a retina with anomalies\", \"a retina showing disease in OCT\",\n",
        "    \"an unhealthy retina\", \"abnormal retinal tissue OCT\",\n",
        "    \"retinal degeneration in OCT\", \"retinal disease scan\"\n",
        "]\n",
        "\n",
        "\n",
        "def buildTextEmbedding(promptList):\n",
        "    tokens = clip.tokenize(promptList).to(device)\n",
        "    with torch.no_grad():\n",
        "        feats = clipModel.encode_text(tokens).float()\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.mean(dim=0)\n",
        "\n",
        "\n",
        "normalTextFeat  = buildTextEmbedding(normalTemplates)\n",
        "anomalyTextFeat = buildTextEmbedding(anomalyTemplates)\n",
        "textFeatures = torch.stack([normalTextFeat, anomalyTextFeat], dim=0).float().to(device)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Zero-shot classification functions\n",
        "# ===============================================================\n",
        "def computeZeroShotScore(images):\n",
        "    with torch.no_grad():\n",
        "        imgFeat = clipModel.encode_image(images).float()\n",
        "        imgFeat = imgFeat / imgFeat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = imgFeat @ textFeatures.T\n",
        "        anomalyScore = logits[:, 1] - logits[:, 0]\n",
        "        return anomalyScore\n",
        "\n",
        "\n",
        "def evaluateZeroShot(loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            logits = computeZeroShotScore(images)\n",
        "            preds = (logits > 0).long().cpu()\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def evaluateZeroShotAUC(loader):\n",
        "    allScores, allLabels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            score = computeZeroShotScore(images)\n",
        "            allScores.extend(score.cpu().numpy())\n",
        "            allLabels.extend(labels.numpy())\n",
        "\n",
        "    return roc_auc_score(allLabels, allScores)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# MVFA-inspired adapter (deep MLP + LayerNorm + skip + gate)\n",
        "# ===============================================================\n",
        "class MVFAAdapter(nn.Module):\n",
        "    def __init__(self, dim=512, hidden=1024, gated=True):\n",
        "        super().__init__()\n",
        "        self.gated = gated\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, dim)\n",
        "        )\n",
        "\n",
        "        self.ln = nn.LayerNorm(dim)\n",
        "        self.gate = nn.Parameter(torch.ones(dim)) if gated else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mlp(x)\n",
        "\n",
        "        if self.gated:\n",
        "            out = out * self.gate\n",
        "\n",
        "        out = self.ln(out + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Few-shot sampling (balanced, k per class)\n",
        "# ===============================================================\n",
        "def getFewShotSamples(dataset, k):\n",
        "    normal = [s for s in dataset.samples if s[1] == 0][:k]\n",
        "    anomaly = [s for s in dataset.samples if s[1] == 1][:k]\n",
        "    return normal + anomaly\n",
        "\n",
        "\n",
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, sampleList):\n",
        "        self.samples = sampleList\n",
        "        self.preprocess = clipPreprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        return self.preprocess(img), label\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Few-shot adapter training\n",
        "# ===============================================================\n",
        "def trainAdapter(k=4, epochs=10, lr=1e-4):\n",
        "    sampleList = getFewShotSamples(trainDs, k)\n",
        "    fsDs = FewShotDataset(sampleList)\n",
        "    fsLoader = DataLoader(fsDs, batch_size=4, shuffle=True)\n",
        "\n",
        "    adapter = MVFAAdapter(dim=clipModel.visual.output_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(adapter.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in fsLoader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                imgFeat = clipModel.encode_image(images).float()\n",
        "\n",
        "            adapted = adapter(imgFeat)\n",
        "            logits = adapted @ textFeatures.T\n",
        "\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return adapter\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Evaluating adapter + fusion scoring\n",
        "# ===============================================================\n",
        "def computeFewShotScore(images, adapter):\n",
        "    with torch.no_grad():\n",
        "        imgFeat = clipModel.encode_image(images).float()\n",
        "        adapted = adapter(imgFeat)\n",
        "        logits = adapted @ textFeatures.T\n",
        "        score = logits[:, 1] - logits[:, 0]\n",
        "        return score\n",
        "\n",
        "\n",
        "def evaluateAdapter(loader, adapter):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.cpu()   # moving labels to CPU\n",
        "\n",
        "            score = computeFewShotScore(images, adapter).cpu()\n",
        "            preds = (score > 0).long()\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def evaluateAdapterAUC(loader, adapter):\n",
        "    allScores, allLabels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            score = computeFewShotScore(images, adapter).cpu()\n",
        "            allScores.extend(score.numpy())\n",
        "            allLabels.extend(labels.numpy())\n",
        "\n",
        "    return roc_auc_score(allLabels, allScores)\n",
        "\n",
        "\n",
        "\n",
        "def evaluateFusionScore(loader, adapter, alpha=0.5):\n",
        "    allScores, allLabels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            scoreZS = computeZeroShotScore(images).cpu()\n",
        "            scoreFS = computeFewShotScore(images, adapter).cpu()\n",
        "\n",
        "            fused = alpha * scoreZS + (1 - alpha) * scoreFS\n",
        "\n",
        "            allScores.extend(fused.numpy())\n",
        "            allLabels.extend(labels.numpy())\n",
        "\n",
        "    return roc_auc_score(allLabels, allScores)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# Running experiments\n",
        "# ===============================================================\n",
        "zeroAcc = evaluateZeroShot(testLoader)\n",
        "zeroAUC = evaluateZeroShotAUC(testLoader)\n",
        "\n",
        "print(f\"\\nZero-Shot Accuracy: {zeroAcc:.4f}\")\n",
        "print(f\"Zero-Shot AUC: {zeroAUC*100:.2f}%\\n\")\n",
        "\n",
        "for k in [2, 4, 8]:\n",
        "    print(f\"===== FEW-SHOT (k={k}) =====\")\n",
        "\n",
        "    adapter = trainAdapter(k=k, epochs=10)\n",
        "\n",
        "    fsAcc = evaluateAdapter(testLoader, adapter)\n",
        "    fsAUC = evaluateAdapterAUC(testLoader, adapter)\n",
        "    fusionAUC = evaluateFusionScore(testLoader, adapter)\n",
        "\n",
        "    print(f\"Few-Shot Accuracy: {fsAcc:.4f}\")\n",
        "    print(f\"Few-Shot AUC: {fsAUC*100:.2f}%\")\n",
        "    print(f\"Fusion AUC: {fusionAUC*100:.2f}%\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxmfoHC6sv_2",
        "outputId": "333e1462-1a72-4a51-e548-c685d0ba947e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 40\n",
            "Test samples: 968\n",
            "\n",
            "Zero-Shot Accuracy: 0.5496\n",
            "Zero-Shot AUC: 23.76%\n",
            "\n",
            "===== FEW-SHOT (k=2) =====\n",
            "Epoch 1/10 | Loss: 0.6820\n",
            "Epoch 2/10 | Loss: 0.6753\n",
            "Epoch 3/10 | Loss: 0.6690\n",
            "Epoch 4/10 | Loss: 0.6629\n",
            "Epoch 5/10 | Loss: 0.6567\n",
            "Epoch 6/10 | Loss: 0.6503\n",
            "Epoch 7/10 | Loss: 0.6438\n",
            "Epoch 8/10 | Loss: 0.6368\n",
            "Epoch 9/10 | Loss: 0.6296\n",
            "Epoch 10/10 | Loss: 0.6219\n",
            "Few-Shot Accuracy: 0.3874\n",
            "Few-Shot AUC: 31.97%\n",
            "Fusion AUC: 31.73%\n",
            "\n",
            "===== FEW-SHOT (k=4) =====\n",
            "Epoch 1/10 | Loss: 0.6783\n",
            "Epoch 2/10 | Loss: 0.6835\n",
            "Epoch 3/10 | Loss: 0.6729\n",
            "Epoch 4/10 | Loss: 0.6540\n",
            "Epoch 5/10 | Loss: 0.6634\n",
            "Epoch 6/10 | Loss: 0.6551\n",
            "Epoch 7/10 | Loss: 0.6625\n",
            "Epoch 8/10 | Loss: 0.6399\n",
            "Epoch 9/10 | Loss: 0.6449\n",
            "Epoch 10/10 | Loss: 0.6103\n",
            "Few-Shot Accuracy: 0.6271\n",
            "Few-Shot AUC: 66.48%\n",
            "Fusion AUC: 65.50%\n",
            "\n",
            "===== FEW-SHOT (k=8) =====\n",
            "Epoch 1/10 | Loss: 0.7037\n",
            "Epoch 2/10 | Loss: 0.6706\n",
            "Epoch 3/10 | Loss: 0.7079\n",
            "Epoch 4/10 | Loss: 0.6694\n",
            "Epoch 5/10 | Loss: 0.6839\n",
            "Epoch 6/10 | Loss: 0.6642\n",
            "Epoch 7/10 | Loss: 0.6592\n",
            "Epoch 8/10 | Loss: 0.6375\n",
            "Epoch 9/10 | Loss: 0.6416\n",
            "Epoch 10/10 | Loss: 0.6376\n",
            "Few-Shot Accuracy: 0.6983\n",
            "Few-Shot AUC: 66.72%\n",
            "Fusion AUC: 66.04%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zipPath = \"/content/liver.zip\"\n",
        "dataRoot = \"/content/liver\"\n",
        "\n",
        "# Extracting the retina dataset\n",
        "with zipfile.ZipFile(zipPath, \"r\") as zipRef:\n",
        "    zipRef.extractall(dataRoot)\n",
        "\n",
        "print(\"Dataset is extracted to:\", dataRoot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09VRgPm_vvDu",
        "outputId": "af032da4-cbf0-4394-9061-e653b73c8aee"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset is extracted to: /content/liver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Importing libraries\n",
        "# ================================================================\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import clip\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Setting paths\n",
        "# ================================================================\n",
        "dataRoot = \"/content/liver/liver\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "imgSize = 256  # using 256×256 resolution\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Loading CLIP model\n",
        "# ================================================================\n",
        "clipModel, clipPreprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "clipModel.eval()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Segmentation dataset loader\n",
        "# ================================================================\n",
        "class LiverSegDataset(Dataset):\n",
        "    def __init__(self, rootDir):\n",
        "        self.imgList = []\n",
        "        self.maskList = []\n",
        "\n",
        "        for clsName in os.listdir(rootDir):\n",
        "            clsFolder = os.path.join(rootDir, clsName)\n",
        "            imgFolder = os.path.join(clsFolder, \"img\")\n",
        "            maskFolder = os.path.join(clsFolder, \"anomaly_mask\")\n",
        "\n",
        "            if not os.path.isdir(imgFolder):\n",
        "                continue\n",
        "\n",
        "            for f in os.listdir(imgFolder):\n",
        "                if f.lower().endswith((\"png\", \"jpg\", \"jpeg\")):\n",
        "                    imgPath = os.path.join(imgFolder, f)\n",
        "                    maskPath = os.path.join(maskFolder, f)\n",
        "\n",
        "                    if os.path.exists(maskPath):\n",
        "                        self.imgList.append(imgPath)\n",
        "                        self.maskList.append(maskPath)\n",
        "\n",
        "        self.preprocess = clipPreprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgList)\n",
        "\n",
        "    def loadMask(self, path):\n",
        "        mask = Image.open(path).convert(\"L\").resize((imgSize, imgSize))\n",
        "        mask = np.array(mask)\n",
        "        mask = (mask > 128).astype(np.float32)\n",
        "        return torch.tensor(mask).unsqueeze(0)  # [1,h,w]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.imgList[idx]).convert(\"RGB\")\n",
        "        img = img.resize((imgSize, imgSize))\n",
        "        imgTensor = self.preprocess(img)\n",
        "\n",
        "        maskTensor = self.loadMask(self.maskList[idx])\n",
        "\n",
        "        label = 1 if \"Ungood\" in self.imgList[idx] else 0\n",
        "        return imgTensor, maskTensor, label\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Creating loaders\n",
        "# ================================================================\n",
        "trainPath = os.path.join(dataRoot, \"valid\")\n",
        "testPath  = os.path.join(dataRoot, \"test\")\n",
        "\n",
        "trainDs = LiverSegDataset(trainPath)\n",
        "testDs  = LiverSegDataset(testPath)\n",
        "\n",
        "trainLoader = DataLoader(trainDs, batch_size=8, shuffle=True)\n",
        "testLoader  = DataLoader(testDs, batch_size=8, shuffle=False)\n",
        "\n",
        "print(\"Train images:\", len(trainDs))\n",
        "print(\"Test images:\", len(testDs))\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Multi-prompt text embeddings for classification component\n",
        "# ================================================================\n",
        "normalPrompts = [\n",
        "    \"a healthy liver CT scan\",\n",
        "    \"a normal liver scan\",\n",
        "    \"a liver CT image without abnormalities\",\n",
        "    \"clear liver tissue CT\",\n",
        "    \"normal medical liver CT\"\n",
        "]\n",
        "\n",
        "anomalyPrompts = [\n",
        "    \"an abnormal liver CT scan\",\n",
        "    \"a diseased liver CT\",\n",
        "    \"a liver CT image showing anomalies\",\n",
        "    \"liver lesion CT scan\",\n",
        "    \"CT scan of abnormal liver tissue\"\n",
        "]\n",
        "\n",
        "\n",
        "def buildTextEmbedding(promptList):\n",
        "    tokens = clip.tokenize(promptList).to(device)\n",
        "    with torch.no_grad():\n",
        "        feats = clipModel.encode_text(tokens).float()\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.mean(dim=0)\n",
        "\n",
        "\n",
        "normalTextFeat = buildTextEmbedding(normalPrompts)\n",
        "anomalyTextFeat = buildTextEmbedding(anomalyPrompts)\n",
        "textFeatures = torch.stack([normalTextFeat, anomalyTextFeat], dim=0).to(device)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Zero-shot segmentation (CLIP similarity map)\n",
        "# ================================================================\n",
        "def computeZeroShotSeg(images):\n",
        "    # Running CLIP and extracting patch features\n",
        "    with torch.no_grad():\n",
        "        imgFeat = clipModel.encode_image(images).float()\n",
        "        imgFeat = imgFeat / imgFeat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = imgFeat @ textFeatures.T\n",
        "        anomalyScore = logits[:, 1] - logits[:, 0]  # scalar anomaly confidence\n",
        "\n",
        "        # Creating simple heatmap: repeat score to image size\n",
        "        heatmap = anomalyScore.unsqueeze(-1).unsqueeze(-1)\n",
        "        heatmap = heatmap.repeat(1, imgSize, imgSize)\n",
        "        return heatmap\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Segmentation head (simple UNet-like 1x1 conv + upsample)\n",
        "# ================================================================\n",
        "class SegHead(nn.Module):\n",
        "    def __init__(self, inDim=512):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(inDim, 256, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = x.repeat(1, 1, imgSize, imgSize)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Few-shot segmentation adapter (MVFA-inspired)\n",
        "# ================================================================\n",
        "class SegAdapter(nn.Module):\n",
        "    def __init__(self, dim=512, hidden=1024):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, dim)\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mlp(x)\n",
        "        out = self.ln(out + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Dice loss + BCE loss\n",
        "# ================================================================\n",
        "def diceLoss(pred, mask):\n",
        "    pred = torch.sigmoid(pred)\n",
        "    smooth = 1e-6\n",
        "    inter = (pred * mask).sum()\n",
        "    union = pred.sum() + mask.sum()\n",
        "    return 1 - (2 * inter + smooth) / (union + smooth)\n",
        "\n",
        "\n",
        "def segLoss(pred, mask):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, mask)\n",
        "    dsc = diceLoss(pred, mask)\n",
        "    return bce + dsc\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Training segmentation adapter + head\n",
        "# ================================================================\n",
        "def trainSegmentation(epochs=15, k=None):\n",
        "    adapter = SegAdapter().to(device)\n",
        "    head = SegHead().to(device)\n",
        "    optim = torch.optim.Adam(list(adapter.parameters()) + list(head.parameters()), lr=1e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        runningLoss = 0\n",
        "        for images, masks, labels in trainLoader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                imgFeat = clipModel.encode_image(images).float()\n",
        "\n",
        "            adapted = adapter(imgFeat)\n",
        "            predMask = head(adapted)\n",
        "\n",
        "            loss = segLoss(predMask, masks)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            runningLoss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {runningLoss/len(trainLoader):.4f}\")\n",
        "\n",
        "    return adapter, head\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Evaluation: localization AUC\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Visualization helper\n",
        "# ================================================================\n",
        "def showPrediction(images, masks, predMasks):\n",
        "    for i in range(min(3, len(images))):\n",
        "        img = images[i].permute(1,2,0).cpu().numpy()\n",
        "        m = masks[i][0].cpu().numpy()\n",
        "        p = predMasks[i][0].detach().cpu().numpy()\n",
        "\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.subplot(1,3,1); plt.imshow(img); plt.title(\"Image\")\n",
        "        plt.subplot(1,3,2); plt.imshow(m, cmap=\"gray\"); plt.title(\"GT Mask\")\n",
        "        plt.subplot(1,3,3); plt.imshow(p, cmap=\"jet\"); plt.title(\"Pred Mask\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Running segmentation training & evaluation\n",
        "# ================================================================\n",
        "adapter, head = trainSegmentation(epochs=15)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKhEENY3xIvQ",
        "outputId": "5a8e7106-40bd-404c-b25d-8cd55edd07c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 166\n",
            "Test images: 1493\n",
            "Epoch 1/15 | Loss: 1.2265\n",
            "Epoch 2/15 | Loss: 1.0230\n",
            "Epoch 3/15 | Loss: 1.0196\n",
            "Epoch 4/15 | Loss: 1.0185\n",
            "Epoch 5/15 | Loss: 1.0174\n",
            "Epoch 6/15 | Loss: 1.0164\n",
            "Epoch 7/15 | Loss: 1.0135\n",
            "Epoch 8/15 | Loss: 1.0079\n",
            "Epoch 9/15 | Loss: 1.0041\n",
            "Epoch 10/15 | Loss: 0.9999\n",
            "Epoch 11/15 | Loss: 0.9989\n",
            "Epoch 12/15 | Loss: 0.9965\n",
            "Epoch 13/15 | Loss: 0.9930\n",
            "Epoch 14/15 | Loss: 0.9990\n",
            "Epoch 15/15 | Loss: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateSegAUC(loader, adapter, head):\n",
        "    allScores, allLabels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks, labels in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            imgFeat = clipModel.encode_image(images).float()\n",
        "            adapted = adapter(imgFeat)\n",
        "            predMask = torch.sigmoid(head(adapted))\n",
        "\n",
        "            # ===== Downsampling masks & predictions to 128×128 =====\n",
        "            predMask128 = F.interpolate(predMask, size=(128, 128), mode=\"bilinear\").cpu().numpy()\n",
        "            mask128     = F.interpolate(masks, size=(128, 128), mode=\"nearest\").cpu().numpy()\n",
        "\n",
        "            # Flattening each image's pixels\n",
        "            predFlat = predMask128.reshape(predMask128.shape[0], -1)\n",
        "            maskFlat = mask128.reshape(mask128.shape[0], -1)\n",
        "\n",
        "            for p, m in zip(predFlat, maskFlat):\n",
        "                allScores.extend(p)\n",
        "                allLabels.extend(m)\n",
        "\n",
        "    return roc_auc_score(allLabels, allScores)\n",
        "\n",
        "auc = evaluateSegAUC(testLoader, adapter, head)\n",
        "print(\"\\nSegmentation AUC:\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PvVMeC4ydj9",
        "outputId": "99a7d981-5b56-4cd1-f478-eac2637aac1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation AUC: 0.875160833531623\n"
          ]
        }
      ]
    }
  ]
}